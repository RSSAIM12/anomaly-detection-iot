from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
from pyspark.sql.functions import from_json, col, current_timestamp
from pyspark.sql.types import StructType, StructField, DoubleType, StringType

# === NOUVEAU ===
from influxdb import InfluxDBClient

# === CrÃ©ation SparkSession ===
spark = SparkSession.builder \
    .appName("KafkaSparkStreamingAnomalyDetection") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# === Chargement du modÃ¨le entraÃ®nÃ© ===
model = PipelineModel.load("model/")

# === DÃ©finition du schÃ©ma Kafka ===
# === DÃ©finition du schÃ©ma Kafka ===
schema = StructType([
    StructField("timestamp", StringType()),  # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¬Ø§ÙŠØ© Ù…Ù† CSV
    StructField("temperature", DoubleType()),
    StructField("pressure", DoubleType()),
    StructField("vibration", DoubleType()),
    StructField("humidity", DoubleType()),
    StructField("equipment", StringType()),
    StructField("location", StringType()),
    StructField("faulty", DoubleType())
])



# === Lecture du flux Kafka ===
kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "iot-topic") \
    .option("startingOffsets", "latest") \
    .load()

# === Conversion JSON + sÃ©lection colonnes ===
value_df = kafka_df.selectExpr("CAST(value AS STRING) as json") \
    .select(from_json(col("json"), schema).alias("data")) \
    .select("data.*")

# === Application du modÃ¨le ===
features_df = model.transform(value_df)

# === Extraction des anomalies ===
anomalies = features_df.filter(features_df.prediction == 1) \
    .withColumn("timestamp", current_timestamp())

# === ðŸ”¥ Fonction pour Ã©crire vers InfluxDB ===
def save_to_influxdb(df, epoch_id):
    client = InfluxDBClient(host='localhost', port=8086, database='iot_db')
    json_body = []
    for row in df.toLocalIterator():
        print(row) 
        json_body.append({
            "measurement": "anomalies",
            "fields": {
                "temperature": row["temperature"],
                "pressure": row["pressure"],
                "vibration": row["vibration"],
                "humidity": row["humidity"],
                "equipment":row["equipment"],
                "faulty": row["faulty"],
                "location":row["location"]
            },
            "time": row["timestamp"].isoformat()
        })

    client.write_points(json_body)
    client.close()

# === Ã‰criture vers InfluxDB (streaming) ===
# Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ù†Ø­Ùˆ InfluxDB
anomalies.drop("features", "rawPrediction", "probability").writeStream \
    .foreachBatch(save_to_influxdb) \
    .outputMode("append") \
    .option("checkpointLocation", "hdfs://localhost:9000/user/sanaa/checkpoint_influx/") \
    .start()

# Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ù†Ø­Ùˆ HDFS (Ø¨ØµÙŠØºØ© CSV)
# Ù†Ø­ÙŠØ¯Ùˆ Ø§Ù„Ø¹Ù…ÙˆØ¯ "features" Ù‚Ø¨Ù„ Ø§Ù„ÙƒØªØ§Ø¨Ø©
anomalies.drop("features", "rawPrediction", "probability").writeStream \
    .format("csv") \
    .option("path", "hdfs://localhost:9000/user/sanaa/anomalies_output/") \
    .option("checkpointLocation", "hdfs://localhost:9000/user/sanaa/checkpoint_hdfs/") \
    .outputMode("append") \
    .start() \
    .awaitTermination()
# === Vider InfluxDB measurement ===

